### 同步能力视图总览


| 组件            | 用途         | 是否涉及数据      | 是否容灾关键因素 |
| ------------- | ---------- | ----------- | -------- |
| yarn          | 资源管理       | 不涉及         | 非        |
| zookeeper     | 协同管理       | 不涉及         | 非        |
| kerberos      | 安全认证       | 不涉及         | 非        |
| Ranger        | 权限认证       | 权限policy数据  | 是        |
| Kafka         | 实时数据传输     | 主题分区数据      | 是        |
| Loader        | 数据加载       |             |          |
| Flume         | 日志采集       |             |          |
| HDFS          | 文件管理       | 所有hadoop数据  | 是        |
| hudi          | 湖格式        | 否（在HDFS层完成） | 否        |
| Alluxio       | 缓存数据       |             |          |
| MemartsCC     | 缓存数据       |             |          |
| Mapreduce     | 批量计算框架     | 否           | 否        |
| Spark         | 批量数据加工     | 否           | 否        |
| Flink         | 实时数据加工     | 否           | 否        |
| Hive          | HQL服务      | 元数据         | 是        |
| Phoenix       | HBase二级索引  | 否           | 否        |
| HetuEngine    | 交互式分析      | 否           | 否        |
| ElasticSearch | 检索引擎       | 是           | 是        |
| Clickhouse    | 实时数据接入实时查询 | 是           | 是        |

| 组件            | 同步工具                | 同步原理                                              | 时效性 | 是否支持异步 | 触发方式      | 一致性评估 | 适用场景                         | 特性            |
| ------------- | ------------------- | ------------------------------------------------- | --- | ------ | --------- | ----- | ---------------------------- | ------------- |
| HDFS          | DistCp              | 生成MapReduce任务传输数据                                 | 批量  | 是      | 一过性任务     | 最终一致性 | HDFS—>HDFS                   | 文件拷贝，支持增全量拷贝  |
| HBase         | Replication         | 主备集群分别启动ReplicationSource、ReplicationSink线程进行数据传输 | 实时  | 是      | 配置开启，常驻任务 | 最终一致性 | HBase—>HBase                 | WAL拷贝，支持增全量拷贝 |
| Kafka         | MirrorMaker         | 从源集群中消费消息，然后将消息发送到目标集群中                           | 实时  | 是      | 配置开启，常驻任务 | 最终一致性 | Kafka—>Kafka                 | 消息传输          |
| ElasticSearch | CCR                 | 批量拉取主集群变更记录并行写入备集群,备集群拉取                          | 实时  | 是      | 配置开启，常驻任务 | 最终一致性 | ElasticSearch—>ElasticSearch | 变更索引拷贝        |
| Hive          | DistCp              | hive数据表通过distcp同步，元数据通过dump或者数据库的同步能力完成           | 批量  | 是      | 一过性任务     | 最终一致性 | Hive—>Hive                   | 数据库同步能力       |
| Clickhouse    | ReplicatedMergeTree | 通过ReplicatedMergeTree引擎（Replicated 系列引擎）实现了副本机制   | 实时  | 是      | 配置开启，常驻任务 | 最终一致性 | Clickhouse—>Clickhouse       | 表副本机制         |
|               |                     |                                                   |     |        |           |       |                              |               |
### 一、HDFS组件

#### DistCp 分布式拷贝

##### **1、定义**：
**DistCp**（分布式拷贝）是用于大规模集群内部和集群之间拷贝的工具。 它使用Map/Reduce实现文件分发，错误处理和恢复，以及报告生成。 它把文件和目录的列表作为map任务的输入，每个任务会完成源列表中部分文件的拷贝。 由于使用了Map/Reduce方法，这个工具在语义和执行上都会有特殊的地方。[^1]
##### **2、同步原理**：
distcp 命令通过提交 mapreduce 程序在 map 阶段进⾏数据传输，基于 block 流读⼀次写⼀次⼤幅度提升拷贝速度。[^3]
![[Pasted image 20250317185307.png]]

##### **3、适用场景**：
Hadoop集群之间，同步HDFS层的数据

##### **4、常用指令**：
（1） 集群间拷贝
```
bash$ hadoop distcp hdfs://nn1:8020/foo/bar hdfs://nn2:8020/bar/foo
```
执行过程：这条命令会把nn1集群的/foo/bar目录下的所有文件或目录名展开并存储到一个临时文件中，这些文件内容的拷贝工作被分配给多个map任务， 然后每个TaskTracker分别执行从nn1到nn2的拷贝操作。注意DistCp使用绝对路径进行操作。
（2）指定多个原集群
```
bash$ hadoop distcp hdfs://nn1:8020/foo/a \  
                    hdfs://nn1:8020/foo/b \  
                    hdfs://nn2:8020/bar/foo
```
执行过程：指定需拷贝的多个源集群地址。注：当从多个源拷贝时，如果两个源冲突，DistCp会停止拷贝并提示出错信息， 如果在目的位置发生冲突，会根据[选项设置](https://hadoop.apache.org/docs/r1.0.4/cn/distcp.html#options)解决。
##### **5、可选参数**

| 标识               | 描述                                                                                                                | 备注                                                                                                |
| ---------------- | ----------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- |
| -p[rbugp]        | Preserve  <br>  r: replication<br>number  <br>  b: block size  <br>  u: user  <br>  g: group  <br>  p: permission | 修改次数不会被保留。并且当指定 -update 时，更新的状态**不**会 被同步，除非文件大小不同（比如文件被重新创建）。                                    |
| -i               | 忽略失败                                                                                                              | 这个选项会比默认情况提供关于拷贝的更精确的统计， 同时它还将保留失败拷贝操作的日志，这些日志信息可以用于调试。最后，如果一个map失败了，但并没完成所有分块任务的尝试，这不会导致整个作业的失败。 |
| -log <logdir>    | 记录日志到 <logdir>                                                                                                    | DistCp为每个文件的每次尝试拷贝操作都记录日志，并把日志作为map的输出。 如果一个map失败了，当重新执行时这个日志不会被保留。                               |
| -m <num_maps>    | 同时拷贝的最大数目                                                                                                         | 指定了拷贝数据时map的数目。请注意并不是map数越多吞吐量越大。                                                                 |
| -overwrite       | 覆盖目标                                                                                                              | 如果一个map失败并且没有使用-i选项，不仅仅那些拷贝失败的文件，这个分块任务中的所有文件都会被重新拷贝。 就像下面提到的，它会改变生成目标路径的语义，所以 用户要小心使用这个选项        |
| -update          | 如果源和目标的大小不一样则进行覆盖                                                                                                 | 像之前提到的，这不是"同步"操作。 执行覆盖的唯一标准是源文件和目标文件大小是否相同；如果不同，则源文件替换目标文件。                                       |
| -f <urilist_uri> | 使用<urilist_uri> 作为源文件列表                                                                                           | 这等价于把所有文件名列在命令行中。 urilist_uri 列表应该是完整合法的URI。                                                      |
##### **6、附录**
（1）Map数目
DistCp会尝试着均分需要拷贝的内容，这样每个map拷贝差不多相等大小的内容。 但因为文件是最小的拷贝粒度，所以配置增加同时拷贝（如map）的数目不一定会增加实际同时拷贝的数目以及总吞吐量。
如果没使用-m选项，DistCp会尝试在调度工作时指定map的数目 为 min (total_bytes / bytes.per.map, 20 * num_task_trackers)， 其中bytes.per.map默认是256MB。
建议对于长时间运行或定期运行的作业，根据源和目标集群大小、拷贝数量大小以及带宽调整map的数目。
（2）不同HDFS版本间的拷贝
对于不同Hadoop版本间的拷贝，用户应该使用HftpFileSystem。 这是一个只读文件系统，所以DistCp必须运行在目标端集群上（更确切的说是在能够写入目标集群的TaskTracker上）。
（3）Map/Reduce和副效应
像前面提到的，map拷贝输入文件失败时，会带来一些副效应。
- 除非使用了-i，任务产生的日志会被新的尝试替换掉。
- 除非使用了-overwrite，文件被之前的map成功拷贝后当又一次执行拷贝时会被标记为 "被忽略"。
- 如果map失败了mapred.map.max.attempts次，剩下的map任务会被终止（除非使用了-i)。
- 如果mapred.speculative.execution被设置为 final和true，则拷贝的结果是未定义的。

---

### 二、HBase组件
#### Replication 分布式拷贝

##### **1、定义**：
**Replication** (TM) 提供了一种在 HBase 部署之间复制数据的方法。它可以用作灾难恢复解决方案，并有助于在 HBase 层提供更高的可用性。它还可以提供更实际的功能；例如，作为一种将编辑内容从面向 Web 的集群轻松复制到“MapReduce”集群的方法，该集群将处理新旧数据并自动发回结果。[^2]
##### **2、同步原理**
HBase 的复制方式是 master-push 方式，即主集群推的方式，主要是因为每个RegionServer都有自己的WAL。 一个master集群可以复制给多个从集群，复制是异步的，运行集群分布在不同的地方，这也意味着从集群和主集群的数据不是完全一致的，它的目标就是最终一致性。

每个RegionServer的 HLog 是 HBase 复制的基础，并且该日志复制时必须将其保存在 HDFS 中。每个 RegionServer 都会从需要复制的最旧的日志中读取，并在 ZooKeeper 中保存当前位置以简化故障恢复。该位置对于每个从属集群可能不同，对于要处理的 HLog 队列也一样。

参与复制的集群可以是不对称大小，主集群将尽“最大努力”通过随机化来平衡从属集群上的复制流。

从 0.92 版本开始，Apache HBase 支持主/主和循环复制以及到多个从属的复制。

![[Pasted image 20250317110926.png]]

按照同步方式可以分类如下[^4]：

**（1）异步Replication**
后台线程异步的读取WAL并复制到备集群，即做异步Replication，正常情况下备集群收到最新写入数据的延迟在**秒**级别。

* **主集群启动线程**：主集群的每个RegionServer进程内部起了一个叫做ReplicationSource的线程来负责Replication
* **备集群启动线程**：的每个RegionServer内部起了一个ReplicationSink的线程来负责接收Replication数据
* **确定同步内容**ReplicationSource记录需要同步的WAL队列，然后不断读取WAL中的内容，同时可以根据Replication的配置做一些过滤，比如是否要复制这个表的数据等
* **RPC调用**：通过replicateWALEntry这个Rpc调用来发送给备集群的RegionServer，备集群的ReplicationSink线程则负责将收到的数据转换为put/delete操作，以batch的形式写入到备集群中

**（2）串行Replication（HBase 2.1引入）**
对于某个Region来说，严格按照主集群的写入顺序复制到备集群，其是一种特殊的Replication。

【注】当源集群发生Region move 或 RegionServer failure 时，在Region move或RegionServer failure之前未推送的HLog条目将由原始RegionServer（用于Region move）或另一个RegionServer（用于RegionServer failure）推送 ，并由现在服务于该区域的RS推送相同区域的新HLog条目，但它们在没有协调的情况下同时推送同一区域的 HLog 条目。
解决方案：确保移动后的Region Server写入操作必须在原Region Server的写入操作复制完成后再进行。


**（3）同步Replication**
异步Replication的最大问题在于复制是存在延迟的，所以在主集群整集群挂掉的情况下，备集群是没有已经写入的完整数据的，会丢失数据。

同步Replication的核心思路就是在写入主集群WAL的同时，在备集群上写入一份RemoteWAL，只有同时在主集群的WAL和备集群的RemoteWAL写入成功了，才会返回给Client说写入成功。

【注】同步Replication是在异步Replication的基础之上的，也就是说异步Replication的复制链路还会继续保留，同时增加了新的写Remote WAL的步骤。
![[Pasted image 20250317150012.png]]

对比异步复制来看，同步复制主要是影响的写路径，从我们的测试结果上来看，大概会有14%的性能下降，后续计划在HBase-20422中进行优化。

##### **3、适用场景**：
HBase到HBase集群之间

##### **4、常用指令**：
（1） 主集群添加复制关系
```
add_peer '100', CLUSTER_KEY => "172.16.10.60:2181:/hbase"
```

（2）建表，并对指定列簇开启Replication
```
create 'tt', {NAME=>'cf', REPLICATION_SCOPE=>'1'}
```

（3）备集群建表
```
create 'tt', {NAME=>'cf'}     
```

（4）通过HBase自带的VerifyReplication工具验证数据
```
hbase org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication -mappers 10 -bandwidth 1024 1 tt
```

##### **5、可选参数**

| 参数                                | 配置                            | 描述                                                                                                                                                                                                                                          |
| --------------------------------- | ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| zookeeper.znode.replication.rs    | /hyperbase1/replication/rs    | 包含主集群 RegionServers 列表 ( /hbase/replication/rs/<region server> )；对每个 RegionServer 节点，都有它要 Replication 数据过去的一个 Per Peer 子节点。<br>在 Peer 子节点内， Hlogs 等待被 Repication ，以这个路径表示: ( /hbase/replication/rs/<region server>/<ClusterId>/<hlogName> ) |
| zookeeper.znode.replication.peers | /hyperbase1/replication/peers | 每个 Peer 有一个子节点(例如： /hbase/replication/peers/<ClusterID> )，包含能够连接到 peer 的 zookeeper 地址。 一个hbase 表可以同时replication到多个peer。                                                                                                                     |
| zookeeper.znode.replication       | /hyperbase1/replication       | 包含 HBase replication state information 的根节点                                                                                                                                                                                                 |
| zookeeper.znode.parent            | /hyperbase{num.}              | Hyperbase在zookeeper上znode的名字                                                                                                                                                                                                                |
| replication.sleep.before.failover | 1                             | 重新尝试同步已经死掉的 region server的WAL队列的时间，单位：ms                                                                                                                                                                                                    |
| replication.executor.workers      | 1                             | 每个 region server 应尝试同时进行故障切换的 region server 的数量                                                                                                                                                                                             |
##### **6、附录**

（1）replication不会根据实际插入的顺序来进行，只保证和master集群最终一致性。
（2）主集群对于同步的数据大小和个数采用默认值较大，容易导致备集群内存被压垮。建议配置减少每次同步数据的大小。replication.source.size.capacity4194304/replication.source.nb.capacity2000
（3）HBase集群间复制时一般通过 CopyTable + Replication共同实现。

---
### 三、Kafka组件
#### MirrorMaker
##### 1、定义
**MirrorMaker** ：Apache Kafka中的一个工具，用于在不同的Kafka集群之间进行数据复制和数据同步。它可以将一个Kafka集群中的主题（topic）的数据复制到另一个Kafka集群中的相应主题，从而实现数据的跨集群复制。

MirrorMaker最初是由LinkedIn开发的，作为其内部使用的数据复制工具。LinkedIn是一个职业社交网络平台，使用Kafka作为其核心的消息传递系统。为了实现数据的跨集群复制和数据同步，LinkedIn团队开发了MirrorMaker。  
MirrorMaker于2012年首次发布，作为LinkedIn的开源项目，成为Apache Kafka的一部分。随着Kafka的快速发展和广泛应用，MirrorMaker也逐渐得到了更多社区的关注和贡献。  
随着时间的推移，MirrorMaker经历了多个版本的更新和改进。在不同的Kafka版本中，MirrorMaker的功能和性能都得到了不断优化和增强。社区也提供了丰富的文档和示例，以帮助用户更好地理解和使用MirrorMaker。  
MirrorMaker的历史可以追溯到Kafka的早期发展阶段，它在Kafka生态系统中扮演着重要的角色，为用户提供了可靠的数据复制解决方案。随着时间的推移，MirrorMaker在Kafka社区中得到了广泛的认可和应用，并成为了许多企业在构建分布式数据处理系统中的重要组成部分。[^5]

##### 2、原理

（1）配置源集群和目标集群：首先，你需要配置源集群和目标集群的连接信息，包括Kafka集群的地址、主题名称等。MirrorMaker需要同时连接到源集群和目标集群。
（2）配置源集群和目标集群：首先，你需要配置源集群和目标集群的连接信息，包括Kafka集群的地址、主题名称等。MirrorMaker需要同时连接到源集群和目标集群。
创建消费者和生产者：MirrorMaker会创建一个或多个消费者，从源集群中读取消息。每个消费者会订阅一个或多个源集群的主题。同时，MirrorMaker也会创建一个或多个生产者，将消息写入目标集群的相应主题。
（3）复制消息：MirrorMaker的消费者会从源集群中读取消息，并将其转发给生产者，生产者将消息写入目标集群。复制过程是异步进行的，即源集群中的数据变化会延迟一段时间后才会在目标集群中出现。
（4）容错和故障处理：MirrorMaker具有一定的容错机制，当源集群或目标集群出现故障或不可用时，它会尽力保证数据的复制和同步。如果源集群的某个分区不可用，MirrorMaker会尝试从其他可用的分区读取消息。如果目标集群的某个分区不可用，MirrorMaker会重试写入操作，直到成功为止。
（5）配置选项：MirrorMaker提供了丰富的配置选项，可以根据需要进行配置。你可以选择复制全部主题或只复制特定的主题，还可以配置复制的速率和数据过滤等。

总之，MirrorMaker通过消费者和生产者的组合，实现了从源集群到目标集群的数据复制和同步。它可以在不同版本的Kafka集群之间进行数据复制，并提供了灵活的配置选项和容错机制，以保证数据的可用性和一致性。

![[Pasted image 20250317185721.jpg]]
##### **3、适用场景**

Kafka集群到Kafka集群

* 灾难恢复与冗余：如果你有一个重要的Kafka集群运行在一个数据中心，并且你希望为其创建一个备份，可以使用MirrorMaker将数据复制到另一个位于不同数据中心的Kafka集群。这样，如果主数据中心发生故障，你可以立即切换到备份数据中心，确保业务的连续性。

* 数据聚合：如果你有多个位于不同地区或部门的Kafka集群，每个集群都在生产重要的数据，你可能希望在一个中心位置进行数据聚合和分析。在这种情况下，可以使用MirrorMaker将所有的Kafka集群的数据复制到一个中心集群。
  
* 跨地域应用：如果你的应用需要在多个地域部署，并需要访问到相同的数据流，那么可以使用MirrorMaker在各个地域之间复制数据。
  
* 数据管道和ETL：可以在源Kafka集群中创建生产者应用，将数据送入管道，然后在目的地Kafka集群中创建消费者应用，对数据进行处理和存储。MirrorMaker可以在这两个集群之间提供数据流。

##### **4、常用指令**：

（1）官方提供的MirrorMaker脚本，用于启动MirrorMaker进程
```
bin/kafka-mirror-maker.sh --consumer.config consumer.properties --producer.config producer.properties --whitelist="^topic.*"
```

##### 5、参数说明

| 参数名                         | 参数含义                                                                                                |
| --------------------------- | --------------------------------------------------------------------------------------------------- |
| whitelist                   | 指定一个正则表达式，指定拷贝源集群中的哪些topic。比如 a\|b 表示拷贝源集群上连个topic的数据a和b。注意，当使用新版本consumer时必须指定该参数。为了方便使用，可以将\|替换为, |
| blacklist                   | 指定一个正则表达式，屏蔽指定topic的拷贝。注意，该参数只使用于老版本consumer                                                        |
| abort.on.send.failure       | 若设置为true，当发送失败时则关闭MirrorMaker                                                                       |
| consumer.config             | 指定MirrorMaker下consumer的属性文件。至少指定bootstrap.server和group.id                                           |
| producer.config             | 指定MirrorMaker下producer的属性文件                                                                         |
| consumer.rebalance.listener | 指定MirrorMaker使用的consumer rebalance监听器                                                               |
| rebalance.listener.args     | 指定MirrorMaker使用的consumer rebalance监听器的参数，与consumer.rebalance.listener一同使用                           |
| message.handler             | 指定消息处理器类。消息处理器在consumer获取消息与producer发送消息之间调用                                                        |
| message.handler.args        | 指定消息处理器类的参数，与message.handler一同使用                                                                    |
| num.streams                 | 指定MirrorMaker线程数。默认是1                                                                               |
| offset.conmmit.interval.ms  | 指定MirrorMaker位移提交间隔，默认值为1分钟<br>help	打印帮助信息                                                          |

---
### 四、ElasticSearch组件

#### CCR
##### 1、定义
 CCR（Cross-Cluster Replication），是Elasticsearch 6.7 版本中引入跨集群复制功能，支持将指定的索引从一个 Elasticsearch 集群复制到一个或多个 Elasticsearch 集群。跨集群复制属于 Elastic Stack 的白金版（PLATINUM）付费功能，需要额外付费才可以使用。[^6]
##### 2、原理
跨集群复制使用主动-被动模型（active-passive model），复制更改的索引称为“追随者索引”（follower Index），被复制的索引称为“领导者索引”（leader index）。当 leader index 收到写入时，follower index 会从远程集群上的 leader index 上基于文档操作实现订阅复制。当配置复制规则时，可以指定特定的索引进行复制；也可以通过 auto-follow pattern 以通配符的形式自动匹配多个索引，创建复制规则后新创建的索引也会自动匹配。

![[Pasted image 20250318142248.png]]

follower index 是被动的，它可以服务于读取请求，但不能接受写入请求，只有 leader index 可以接受写入请求。

复制过程主要分为两个阶段(对应ES写入的两个阶段，复制落地到磁盘的segment和复制写入缓存区还未落地到磁盘的数据)

- Step1--复制远程leader集群的**segment**到 本地 follower集群

![[Pasted image 20250318152607.png]]
- Step2--复制远程leader集群的 **operator records** (存在内存缓冲区和translog )到 本地follower集群。
![[Pasted image 20250318152627.png]]
💢当主集群发生宕机时，如果我们想让在备集群中的 follower index 接管服务，允许进行写入请求，需要依次暂停索引的复制，关闭索引，取消跟随 leader index，最后将其重新打开为普通的索引，整套操作下来非常复杂，而且无法进行回切，这也是 CCR 跨集群复制最大的一个问题。
![[Pasted image 20250318144030.png]]

针对上述问题，这里也提供了一种解决方案，那就是双向复制（CCR bi-directional replication）。如下图所示，我们可以在集群 cluster01 上创建索引 `logs-cluster01`，并在集群 cluster02 上复制该索引；在集群 cluster02 上创建索引 `logs-cluster02` ，并在集群 cluster01 上复制该索引。然后在两个集群上创建别名 `logs`  指向索引 `logs-cluster01` 和 `logs-cluster02`，对外可以提供统一的索引名。别名指向的多个索引中，只能有一个索引是允许接收写入请求的，在 cluster01 将索引 `logs-cluster01` 设置为可写，`logs-cluster02` 索引中的数据将会通过 CCR 跨集群复制从集群 cluster02 中获取；集群 cluster02 的别名设置相反。通过以上设置，应用程序在读写的时候都往 `logs`  别名发送请求，假设当集群 cluster01 不可用时，我们可以继续使用集群 cluster02，并且依然是往 `logs` 别名发送请求，无需手动进行切换操作；当集群 cluster01 恢复后，会从上次记录的 checkpoint 继续从集群 cluster02 复制数据，也无需额外的切换操作。

##### 3、适用场景
ElasticSearch实时同步至ElasticSearch

- **灾难恢复（DR）/高可用性（HA）**：如果主群集发生故障，则进行灾难恢复。 辅助群集可以用作热备份
- **地理位置优先**：在 Elasticsearch 中复制数据以更接近用户或应用程序服务器，从而减少延迟。优先就近读取数据，提升性能
- **集中报告**：将数据从大量较小的集群复制回一个中央集群进行报告[^7]

##### 4、常用指令：
（1）开启CCR，Elasticsearch 7.0+ 之后版本已默认开启，无需单独配置。

（2）连接到远程集群
```
PUT /_cluster/settings  
{  
  "persistent" : {  
    "cluster" : {  
      "remote" : {  
        "es01" : {  
          "seeds" : [  
            "es01:9300"   
          ]  
        }  
      }  
    }  
  }  
}
```

（3）创建Follower index
```
PUT /index-1-follower/_ccr/follow?wait_for_active_shards=1  
{  
  "remote_cluster" : "es01",  
  "leader_index" : "index-1"  
}  
  
_# 返回结果_  
{  
  "follow_index_created" : true,  
  "follow_index_shards_acked" : true,  
  "index_following_started" : true  
}
```
##### 5、参数说明
| 参数                                              | 说明                                                                                                         |
| ----------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |
| ccr.indices.recovery.max_bytes_per_sec          | 限制每个节点上的总入站和总出站的远程流量(速度)。(默认值是40mb)，当主集群和从集群都设置这个值后，先达到限制值的先其作用。即如果主集群设置20mb，从集群设置60mb，那么主集群也只会想从集群发送20mb。 |
| ccr.indices.recovery.max_concurrent_file_chunks | 复制文件的并行度，即可以同时复制接个文件，默认值是5，最大值为10.                                                                         |
| ccr.indices.recovery.chunk_size                 | 复制文件一次请求的最大限制，默认值是1mb                                                                                      |
| ccr.indices.recovery.recovery_activity_timeout  | leader等待follower恢复请求的时间，默认值是60s                                                                            |
| ccr.indices.recovery.internal_action_timeout    | 在远程恢复过程中单个网络请求的超时值，默认值是60s                                                                                 |
##### 6、附录

- ​**分片数量与线程数关系**：分片数量直接影响并行复制的线程数，需合理规划分片数以避免资源争用。
- ​**版本兼容性**：需确保 leader 和 follower 集群的 Elasticsearch 版本兼容，否则可能引发线程池调度异常
- ​**网络带宽**：多线程可能加剧跨集群网络负载，需监控带宽使用情况。


### 五、Hive组件

hive的数据包含两部分，一部分是hive数据表，可以通过distcp同步，另一部分是元数据，可通过dump方式生成文件，再通过distcp同步，或者基于元数据的数据库能力进行同步均可，不再介绍

### 六、Clickhouse组件

#### ReplicatedMergeTree引擎
##### 1、定义
 ClickHouse 采用 Multi - Master 的多主架构，集群中的每个节点角色对等，客户端访问任意一个节点都能达到相同的结果。这种多主架构有许多优势，例如对等的角色使系统架构变的更加简单，不再区分主控节点、数据节点和计算节点，集群中所有节点的功能相同。正因如此，ClickHouse 天然规避了单点故障的问题，非常适用于多数据中心、异地多活等场景。[^8]

ClickHouse利用ZooKeeper，通过ReplicatedMergeTree引擎（Replicated 系列引擎）实现了副本机制。副本机制是多主架构，可以将INSERT语句发送给任意一个副本，其余副本会进行数据的异步复制。

##### 2、原理

副本机制功能：
- ClickHouse副本机制的设计可以最大限度的减少网络数据传输，用以在不同的数据中心进行同步，可以用来建设多数据中心、异地多活的集群架构。
- 副本机制是实现：高可用（HA）、负载均衡（Load Balance）、迁移/升级（Migration/Upgrade）功能的基础。
- 高可用：系统会监视副本数据的同步情况，识别故障节点，并在节点恢复正常时进行故障恢复，保证服务整体高可用。
![[Pasted image 20250318183444.png]]
ReplicateMergeTree可以通过和zk结合，把数据同步到对应的副本节点中，而且同步是相互的，也就是说从A节点写入的数据会同步到B节点，从B节点写入的数据也会写入到A节点中，典型的Mul-Master架构。通过一个分片多个副本的形式可以分摊读和写的负载，我们看一下同步的原理：

（1）insert数据：假设A节点进行数据插入，首先A节点本地会创建一个新的目录分区，然后他会把这个分区目录的信息推送到zk的log目录节点下，目的是为了通知B节点来获取该分区的数据，B节点会监听zk的log节点的变更通知，让得知是要去A节点同步数据分区数据时，他首先先把这个消息发送到他对应的zk上面的任务执行队列，B本身会按照zk上面的执行队列的顺序顺序执行各个操作，自然当获取到是要去A同步数据分区数据时，他会和A节点建立数据连接，希望从A那里下载到该分区数据，等到A返回对应的分区数据时，B会在本地创建一个名称一模一样的目录分区数据,自此insert的数据完成了同步。
![[Pasted image 20250318184230.png]]
Insert数据插入操作时多个副本之间是有存在副本一致性问题的，也就是同步期间副本0和副本1的数据是不一致的，所以对于使用multi-master进行写多个副本的操作时，需要意识到在一定的时间内多个副本之间的数据是不一致的，所以选择这种方案需要考虑到数据不一致的问题

b. merge合并数据: 这个操作需要从A和B中通过zk选举出主节点，假设A是主节点，B是副节点，当B执行optimize table操作合并数据分区时，B首先和主节点A直接建立连接，告诉A主副本负责制定merge合并计划，当A收到B的制定merge计划的请求后，A制定了merge计划，比如把2020_0_0_0和2020_2_2_1合并成2020_0_2_2，他会把这个执行计划推送到zk的log节点中，这样A和B监听到zk的log节点执行计划后会分别把这个执行计划推送到各自在zk上的queue任务队列中，然后A和B各自执行各自的任务队列操作时，就会各自按照执行计划merge各自的分区
![[Pasted image 20250318184300.png]]
c. update/delete数据：假设A节点进行数据删除操作Alter table XX delete id=100，首先A节点会把这个操作组合成一个消息（包含mutationid）发送到zk中，此后节点A和B会监听zk中的mutation节点的变更，假设此时B节点是主节点，所以B节点会响应这个数据变更的消息并指定具体的数据更新计划，比如目录文件从202209_0_0_1在进行了delete具体操作后变成202209_0_0_1_mutationid,然后B节点会发送这个具体的数据更新消息到zk中，此时A和B节点会同时监听到具体的数据变更消息，然后在各自的本地磁盘中进行数据的修改/删除操作,自此update/delete的数据完成了同步。
![[Pasted image 20250318184309.png]]
update/delete数据修改操作时多个副本之间是有存在副本一致性问题的，也就是同步期间副本0和副本1的数据是不一致的，所以对于使用multi-master进行修改删除多个副本的操作时，需要意识到在一定的时间内多个副本之间的数据是不一致的，所以选择这种方案需要考虑到数据不一致的问题

整个执行过程中zk只是作为消息通知的手段，clickhouse表数据的传输并没有通过zk进行，zk的压力其实不大，此外，对于ReplicateMergeTree表的查询操作并不需要zk的参与。

##### 3、适用场景
Clickhouse多主架构


##### 4、常用指令[^9]
```
CREATE TABLE IF NOT EXISTS test.events_local ON CLUSTER '{cluster}' (
  ts_date Date,
  ts_date_time DateTime,
  uzser_id Int64,
  event_type String,
  site_id Int64,
  groupon_id Int64,
  category_id Int64,
  merchandise_id Int64,
  search_text String
  -- A lot more columns...
)
ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/test/events_local','{replica}')
PARTITION BY ts_date
ORDER BY (ts_date,toStartOfHour(ts_date_time),site_id,event_type)
SETTINGS index_granularity = 8192;
```

##### 5、参数说明

| 参数             | 介绍                                      |
| -------------- | --------------------------------------- |
| `zoo_path`     | ZooKeeper 路径。相同的 ZooKeeper 路径对应于相同的数据库。 |
| `shard_name`   | 分片名称。数据库副本按 `shard_name` 分组到分片中。        |
| `replica_name` | 副本名称。相同分片的所有副本名称必须不同。                   |
对于[ReplicatedMergeTree](https://clickhouse.com/docs/zh/engines/table-engines/mergetree-family/replication)表，如果没有提供参数，则使用默认参数：`/clickhouse/tables/{uuid}/{shard}`和`{replica}`。这些可以在服务器设置中更改 [default_replica_path](https://clickhouse.com/docs/zh/operations/server-configuration-parameters/settings#default_replica_path) 和 [default_replica_name](https://clickhouse.com/docs/zh/operations/server-configuration-parameters/settings#default_replica_name)。宏 `{uuid}` 被展开为表的 uuid，`{shard}` 和`{replica}` 被展开为来自服务器配置的值，而不是数据库引擎参数中的值。但在将来，可以使用副本数据库的 `shard_name` 和 `replica_name`。[^10]

##### 6、附录
- ClickHouse容灾保护的范围是基于复制表创建的分布式表和物化视图，即对分布式表、物化视图及相关的复制表可以完成容灾保护同步数据到容灾集群。
- ![[assets/大数据组件数据同步能力/1751852687392.png]]


[^1]: https://hadoop.apache.org/docs/r1.0.4/cn/distcp.html

[^2]: https://svn-master.apache.org/repos/asf/hbase/hbase.apache.org/trunk/0.94/replication.html

[^3]: https://yampery.github.io/2019/01/29/distcp/

[^4]: https://www.cnblogs.com/caoweixiong/p/13606732.html

[^5]: https://www.ctyun.cn/developer/article/442989040070725

[^6]: https://www.infvie.com/ops-notes/elasticsearch-multiple-cross-machine-rooms.html

[^7]: https://www.cnblogs.com/tgzhu/p/17090475.html

[^8]: https://www.dbkuaizi.com/archives/207.html

[^9]: https://blog.csdn.net/nazeniwaresakini/article/details/105858390

[^10]: https://clickhouse.com/docs/zh/engines/database-engines/replicated
